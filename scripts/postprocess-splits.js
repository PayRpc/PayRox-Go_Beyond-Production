#!/usr/bin/env node

const fs = require('fs')
const path = require('path')
const argv = require('minimist')(process.argv.slice(2), {
  string: ['dir'],
  alias: { d: 'dir' },
  default: { dir: 'artifacts/splits' }
})

const outDir = path.resolve(argv.dir)
if (!fs.existsSync(outDir)) {
  console.error(`Output dir not found: ${outDir}`)
  process.exit(2)
}

const combinedPath = path.join(outDir, 'combined.json')
if (!fs.existsSync(combinedPath)) {
  console.error(`combined.json not found in ${outDir}`)
  process.exit(1)
}

let combined = {}
try {
  combined = JSON.parse(fs.readFileSync(combinedPath, 'utf8')) || {}
} catch (e) {
  // leave combined as empty object
}

const parts = fs
  .readdirSync(outDir)
  // include generated split files like part_2_split_0.json
  .filter((f) => /^part.*\.json$/i.test(f))
  .map((f) => path.join(outDir, f))

const kept = []
const allSelectors = new Set()
const MAX_FACET_CODE = 24576 // EIP-170 hard limit
const PAYROX_SAFE_FACET_SIZE = 18000 // conservative soft packing target to avoid warnings (lowered)

// Normalize Solidity text to improve AST parsing on generated/malformed files.
// - rejoin identifiers split across lines
// - remove zero-width/control characters
// - collapse repeated blank lines
// - normalize CRLF to LF
function normalizeSolText (text) {
  if (!text || typeof text !== 'string') return text
  // normalize newlines
  let s = text.replace(/\r\n?/g, '\n')
  // remove zero-width and other control characters except common whitespace
  s = s.replace(/[\u200B-\u200F\uFEFF\u00A0]/g, '')
  // rejoin accidental mid-identifier newlines: a\n b -> ab
  s = s.replace(/([A-Za-z0-9_$])\n\s*([A-Za-z0-9_$])/g, '$1$2')
  // ensure closing braces and semicolons aren't merged into identifiers
  s = s.replace(/([;{}])\n\s*([A-Za-z0-9_$])/g, '$1\n$2')
  // collapse 3+ blank lines into two
  s = s.replace(/\n{3,}/g, '\n\n')
  // trim trailing spaces on each line
  s = s
    .split('\n')
    .map((l) => l.replace(/[ \t]+$/g, ''))
    .join('\n')
  return s
}

for (const jsonFile of parts) {
  const base = path.basename(jsonFile, '.json')
  const solFile = path.join(outDir, base + '.sol')
  const info = JSON.parse(fs.readFileSync(jsonFile, 'utf8'))
  const selectors = Array.isArray(info.selectors) ? info.selectors : []

  if (selectors.length === 0) {
    // delete empty part files
    try {
      fs.unlinkSync(jsonFile)
    } catch {
      // Ignore deletion errors
    }
    try {
      fs.unlinkSync(solFile)
    } catch {
      // Ignore deletion errors
    }
    console.log(`ðŸ§¹ removed empty ${base}.{sol,json}`)
    continue
  }

  selectors.forEach((s) => allSelectors.add(String(s).toLowerCase()))
  kept.push({
    name: info.name || base,
    file: base + '.sol',
    json: base + '.json',
    selectors
  })
}

// Helper: attempt to split an oversized sol file into smaller parts by extracting function bodies
function splitOversizedPart (solPath, jsonInfo, recDepth = 0) {
  const solText = fs.readFileSync(solPath, 'utf8')
  const stat = fs.statSync(solPath)
  if (stat.size <= MAX_FACET_CODE) return null // nothing to do

  console.log(
    `ðŸ”§ splitting oversized ${path.basename(solPath)} (${stat.size} bytes)`
  )

  // Find contract header/footer
  const contractRe = /(contract|library|interface)\s+([A-Za-z0-9_]+)[^{]*\{/g
  const m = contractRe.exec(solText)
  if (!m) {
    // Fallback: try AST-based splitter (node script) if available
    const nodeSplitter = path.resolve(
      'scripts',
      'tools',
      'ast',
      'split-facets.js'
    )
    if (fs.existsSync(nodeSplitter)) {
      try {
        // Write a temporary cleaned copy where accidental mid-identifier newlines are rejoined
        const cleaned = normalizeSolText(solText)
        const tmpPath = solPath + '.cleaned.tmp.sol'
        fs.writeFileSync(tmpPath, cleaned, 'utf8')
        const child = require('child_process').spawnSync(
          'node',
          [nodeSplitter, tmpPath],
          {
            encoding: 'utf8',
            timeout: 20000
          }
        )
        if (child.status === 0 && child.stdout) {
          const data = JSON.parse(child.stdout)
          // Map AST fragments into same shape as generated parts
          let generated = data.map((frag, idx) => {
            const newNameBase = `${path.basename(solPath, '.sol')}_ast_${idx}`
            const newSolPath = path.join(outDir, newNameBase + '.sol')
            const newJsonPath = path.join(outDir, newNameBase + '.json')
            fs.writeFileSync(newSolPath, frag.code, 'utf8')
            const meta = {
              name: frag.name || newNameBase,
              selectors: frag.selectors || [],
              size: frag.size || Buffer.byteLength(frag.code, 'utf8')
            }
            fs.writeFileSync(
              newJsonPath,
              JSON.stringify(meta, null, 2),
              'utf8'
            )
            return {
              file: path.basename(newSolPath),
              json: path.basename(newJsonPath),
              selectors: meta.selectors
            }
          })
          // If generated parts are still large, try recursive splitting (limit depth)
          if (recDepth < 3) {
            const finalGen = []
            for (const g of generated) {
              try {
                const gsol = path.join(outDir, g.file)
                const gst = fs.statSync(gsol)
                if (gst.size > PAYROX_SAFE_FACET_SIZE) {
                  // attempt to split further
                  const info = { name: g.file, selectors: g.selectors || [] }
                  const sub = splitOversizedPart(gsol, info, recDepth + 1)
                  if (sub && sub.length > 0) {
                    // remove the large piece
                    try {
                      fs.unlinkSync(gsol)
                    } catch (err) {
                      // ignore removal error
                    }
                    try {
                      fs.unlinkSync(path.join(outDir, g.json))
                    } catch (err) {
                      // ignore
                    }
                    sub.forEach((s) => finalGen.push(s))
                    continue
                  }
                }
              } catch (e) {
                // ignore
              }
              finalGen.push(g)
            }
            generated = finalGen
          }
          try {
            fs.unlinkSync(tmpPath)
          } catch (err) {
            // ignore
          }
          return generated
        }
      } catch (e) {
        console.warn(
          `WARN: AST splitter failed for ${solPath}: ${e?.message}`
        )
      }
    }
    console.warn(
      `WARN: couldn't find contract header in ${solPath}; skipping split`
    )
    return null
  }

  const headerStart = m.index
  const openIdx = solText.indexOf('{', headerStart)

  // find matching closing brace for contract
  let depth = 0
  let contractClose = -1
  for (let i = openIdx; i < solText.length; i++) {
    if (solText[i] === '{') depth++
    else if (solText[i] === '}') {
      depth--
      if (depth === 0) {
        contractClose = i
        break
      }
    }
  }

  if (contractClose === -1) {
    // Try AST-based splitter as a fallback when brace matching fails
    const nodeSplitter = path.resolve(
      'scripts',
      'tools',
      'ast',
      'split-facets.js'
    )
    if (fs.existsSync(nodeSplitter)) {
      try {
        const cleaned = normalizeSolText(solText)
        const tmpPath = solPath + '.cleaned.tmp.sol'
        fs.writeFileSync(tmpPath, cleaned, 'utf8')
        const child = require('child_process').spawnSync(
          'node',
          [nodeSplitter, tmpPath],
          {
            encoding: 'utf8',
            timeout: 20000
          }
        )
        if (child.status === 0 && child.stdout) {
          const data = JSON.parse(child.stdout)
          const generated = data.map((frag, idx) => {
            const newNameBase = `${path.basename(solPath, '.sol')}_ast_${idx}`
            const newSolPath = path.join(outDir, newNameBase + '.sol')
            const newJsonPath = path.join(outDir, newNameBase + '.json')
            fs.writeFileSync(newSolPath, frag.code, 'utf8')
            const meta = {
              name: frag.name || newNameBase,
              selectors: frag.selectors || [],
              size: frag.size || Buffer.byteLength(frag.code, 'utf8')
            }
            fs.writeFileSync(
              newJsonPath,
              JSON.stringify(meta, null, 2),
              'utf8'
            )
            return {
              file: path.basename(newSolPath),
              json: path.basename(newJsonPath),
              selectors: meta.selectors
            }
          })
          try {
            fs.unlinkSync(tmpPath)
          } catch (err) {
            // ignore
          }
          return generated
        }
      } catch (e) {
        console.warn(
          `WARN: AST splitter failed for ${solPath}: ${e?.message}`
        )
      }
    }
    console.warn(
      `WARN: couldn't find contract end in ${solPath}; attempting best-effort tail-scan`
    )
    // Best-effort: treat rest of file as body (no footer) and continue
    // Variables will be redeclared below with proper values
  }

  const header = solText.slice(0, openIdx + 1)
  const footer = solText.slice(
    contractClose >= 0 ? contractClose : solText.length
  )
  const body = solText.slice(
    openIdx + 1,
    contractClose >= 0 ? contractClose : solText.length
  )

  // Find functions and interface declarations
  const funcRe = /function\s+([A-Za-z0-9_]+)\s*\([^)]*\)\s*([^;{]*)({|;)/g
  const funcs = []
  let fm
  while ((fm = funcRe.exec(body)) !== null) {
    const fStart = fm.index
    const pre = fm[0]
    const name = fm[1]
    const hasBody = fm[3] === '{'
    if (hasBody) {
      // find matching brace in body starting from fStart + offset
      const idx = fStart + body.indexOf('{', fStart)
      let d = 0
      let endIdx = -1
      for (let j = idx; j < body.length; j++) {
        if (body[j] === '{') d++
        else if (body[j] === '}') {
          d--
          if (d === 0) {
            endIdx = j
            break
          }
        }
      }
      if (endIdx === -1) continue
      const snippet = body.slice(fStart, endIdx + 1)
      funcs.push({ name, code: snippet })
    } else {
      // interface-style declaration ends with semicolon
      const semi = body.indexOf(';', fStart + pre.length - 1)
      if (semi === -1) continue
      const snippet = body.slice(fStart, semi + 1)
      funcs.push({ name, code: snippet })
    }
  }

  if (funcs.length === 0) {
    console.warn(
      `WARN: no function blocks found in ${solPath}; skipping split`
    )
    return null
  }

  // Pack functions into chunks so each generated part stays under PAYROX_SAFE_FACET_SIZE
  let generated = []
  const headerBytes = Buffer.byteLength(header, 'utf8')
  const footerBytes = Buffer.byteLength(footer, 'utf8')
  const PACK_TARGET = PAYROX_SAFE_FACET_SIZE
  const chunks = []
  let cur = []
  let curBytes = headerBytes + footerBytes
  for (const fn of funcs) {
    const fBytes = Buffer.byteLength(fn.code, 'utf8')
    if (curBytes + fBytes > PACK_TARGET) {
      if (cur.length === 0) {
        // single function too large; put it alone (cannot split)
        chunks.push([fn])
        cur = []
        curBytes = headerBytes + footerBytes
      } else {
        chunks.push(cur)
        cur = [fn]
        curBytes = headerBytes + footerBytes + fBytes
      }
    } else {
      cur.push(fn)
      curBytes += fBytes
    }
  }
  if (cur.length > 0) chunks.push(cur)

  for (let i = 0; i < chunks.length; i++) {
    const slice = chunks[i]
    const newBody = slice.map((f) => f.code).join('\n\n')
    const newNameBase = `${path.basename(solPath, '.sol')}_split_${i}`
    const newSolPath = path.join(outDir, newNameBase + '.sol')
    const newJsonPath = path.join(outDir, newNameBase + '.json')
    const newSol = header + '\n' + newBody + '\n' + footer
    fs.writeFileSync(newSolPath, newSol, 'utf8')
    const selectors = (jsonInfo.selectors || []).slice(
      i * Math.ceil(jsonInfo.selectors.length / chunks.length),
      (i + 1) * Math.ceil(jsonInfo.selectors.length / chunks.length)
    )
    const meta = {
      name: jsonInfo.name || newNameBase,
      selectors,
      size: Buffer.byteLength(newSol, 'utf8')
    }
    fs.writeFileSync(newJsonPath, JSON.stringify(meta, null, 2), 'utf8')
    generated.push({
      file: path.basename(newSolPath),
      json: path.basename(newJsonPath),
      selectors
    })
    console.log(
      `âœ‚ created ${path.basename(newSolPath)} (${meta.size} bytes, ${selectors.length} selectors)`
    )
  }
  // If any generated part is still large, try recursive split (limited depth)
  if (recDepth < 3) {
    const finalGen = []
    for (const g of generated) {
      try {
        const gsol = path.join(outDir, g.file)
        const gst = fs.statSync(gsol)
        if (gst.size > PAYROX_SAFE_FACET_SIZE) {
          const info = { name: g.file, selectors: g.selectors || [] }
          const sub = splitOversizedPart(gsol, info, recDepth + 1)
          if (sub && sub.length > 0) {
            try {
              fs.unlinkSync(gsol)
            } catch (_) {
              // Ignore file deletion errors
            }
            try {
              fs.unlinkSync(path.join(outDir, g.json))
            } catch (_) {
              // Ignore file deletion errors
            }
            sub.forEach((s) => finalGen.push(s))
            continue
          }
        }
      } catch (e) {
        // ignore
      }
      finalGen.push(g)
    }
    generated = finalGen
  }
  return generated
}
// Aggressively ensure a given sol/json are fully split under the PACK_TARGET by calling
// splitOversizedPart repeatedly (safeguarded with a max depth/iterations).
function ensureFullySplit (solPath, jsonInfo, maxIter = 4) {
  const result = []
  const queue = [{ sol: solPath, info: jsonInfo, depth: 0 }]
  while (queue.length > 0) {
    const { sol, info, depth } = queue.shift()
    try {
      const st = fs.statSync(sol)
      if (st.size <= PAYROX_SAFE_FACET_SIZE || depth >= maxIter) {
        // keep as-is
        result.push({ sol, info })
        continue
      }
    } catch (e) {
      // missing file -> skip
      continue
    }

    const gen = splitOversizedPart(sol, info, 0)
    if (!gen || gen.length === 0) {
      // couldn't split further; keep original
      result.push({ sol, info })
      continue
    }

    // remove original
    try {
      fs.unlinkSync(sol)
    } catch (_) {
      // Ignore file deletion errors
    }
    try {
      fs.unlinkSync(path.join(outDir, info?.json ? info.json : ''))
    } catch (_) {
      // Ignore file deletion errors
    }

    // push generated items back to queue to ensure they are small enough
    for (const g of gen) {
      const gsol = path.join(outDir, g.file)
      const gjsonPath = path.join(outDir, g.json || '')
      let ginfo = { name: g.file, selectors: g.selectors || [] }
      // attempt to read meta if available
      try {
        if (gjsonPath && fs.existsSync(gjsonPath)) {
          const meta = JSON.parse(fs.readFileSync(gjsonPath, 'utf8'))
          ginfo = meta
        }
      } catch (_) {
        // Ignore JSON parsing errors
      }
      queue.push({ sol: gsol, info: ginfo, depth: depth + 1 })
    }
  }
  return result
}
// Now, attempt to split any oversized kept parts
const finalParts = []
for (const p of kept) {
  const solPath = path.join(outDir, p.file)
  const jsonPath = path.join(outDir, p.json)
  try {
    const stat = fs.statSync(solPath)
    // Attempt to further split any part larger than our soft packing target so validator warnings go away
    if (stat.size > PAYROX_SAFE_FACET_SIZE) {
      const info = JSON.parse(fs.readFileSync(jsonPath, 'utf8'))
      // aggressively ensure generated parts are under the PACK target
      const ensured = ensureFullySplit(solPath, info, 5)
      if (ensured && ensured.length > 0) {
        // remove original files if they still exist
        try {
          fs.unlinkSync(solPath)
        } catch (e) {
          // Ignore file deletion errors
        }
        try {
          fs.unlinkSync(jsonPath)
        } catch (e) {
          // Ignore file deletion errors
        }

        // add generated parts from ensured results
        ensured.forEach((it) => {
          const fname = path.basename(it.sol)
          const jname = path.basename(fname.replace(/\.sol$/i, '.json'))
          finalParts.push({
            name: (it.info?.name) || fname,
            file: fname,
            json: jname,
            selectors: (it.info?.selectors) || []
          });
          ((it.info?.selectors) || []).forEach((s) =>
            allSelectors.add(String(s).toLowerCase())
          )
        })
        continue
      }
    }
  } catch (e) {
    // ignore and keep original
  }
  finalParts.push(p)
}

const rewritten = {
  ...(typeof combined === 'object' ? combined : {}),
  parts: finalParts.filter((p) => (p.selectors || []).length > 0),
  selectors: [...allSelectors],
  by_part: finalParts
    .filter((p) => (p.selectors || []).length > 0)
    .map((p) => ({ file: p.file, functions: (p.selectors || []).length }))
}

fs.writeFileSync(combinedPath, JSON.stringify(rewritten, null, 2))
console.log(
  `âœ… postprocess complete: kept ${finalParts.length} parts, ${allSelectors.size} selectors`
)
